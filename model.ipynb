{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('features.csv',index_col=0)\n",
    "df_validation = pd.read_csv('validation_features.csv',index_col=0)\n",
    "\n",
    "# Fill all NaN values with 0\n",
    "df = df.fillna(0)\n",
    "df.columns = df.columns.astype(str)\n",
    "\n",
    "df_validation = df_validation.fillna(0)\n",
    "df_validation.columns = df_validation.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF used as training set\n",
    "print('Training set shape:', df.shape)\n",
    "# DF used as validation\n",
    "print('Validation set shape:', df_validation.shape)\n",
    "print()\n",
    "print('Features:', df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = df[['Cleaned Content','Gender']]\n",
    "df_validation_bert = df_validation[['Cleaned Content','Gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id = df[['ID', 'Gender', 'Mapped Name', 'Cleaned Content']]\n",
    "df_validation_id = df_validation[['ID', 'Gender', 'Mapped Name', 'Cleaned Content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ID','Mapped Name','Cleaned Content'],inplace=True)\n",
    "df_validation.drop(columns=['ID','Mapped Name','Cleaned Content'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender_mapped'] = df['Gender'].map({'F': 0, 'M': 1})\n",
    "df_validation['Gender_mapped'] = df_validation['Gender'].map({'F': 0, 'M': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(df.columns.drop(['Gender', 'Gender_mapped']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Group by 'Gender' and calculate descriptive statistics\n",
    "grouped_stats = df.groupby('Gender').describe()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "grouped_stats.columns = ['_'.join(col).strip() for col in grouped_stats.columns.values]\n",
    "\n",
    "# # Export the descriptive statistics to a CSV file\n",
    "# grouped_stats.to_csv('grouped_descriptive_statistics.csv')\n",
    "\n",
    "# print(\"Descriptive statistics have been exported to 'grouped_descriptive_statistics.csv'.\")\n",
    "\n",
    "# Display the statistics by feature\n",
    "for feature in all_features:\n",
    "    print(f\"\\nDescriptive statistics for {feature} by Gender:\")\n",
    "    display_stats = grouped_stats[[f\"{feature}_count\", f\"{feature}_mean\", f\"{feature}_std\", \n",
    "                                   f\"{feature}_min\", f\"{feature}_25%\", f\"{feature}_50%\", \n",
    "                                   f\"{feature}_75%\", f\"{feature}_max\"]]\n",
    "    display(display_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove categorical features\n",
    "df_corr = df.drop('Gender',axis=1)\n",
    "correlation_matrix = df_corr.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only strongly correlated grids\n",
    "\n",
    "# Define the correlation threshold\n",
    "threshold = 0.7\n",
    "\n",
    "# Create a mask for the correlation matrix to keep only correlations above the threshold\n",
    "high_corr = correlation_matrix[(correlation_matrix.abs() >= threshold) & (correlation_matrix.abs() != 1.0)]\n",
    "\n",
    "# Drop rows and columns where all values are NaN (correlations below the threshold)\n",
    "high_corr = high_corr.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "\n",
    "# Plot the filtered correlation matrix\n",
    "plt.figure(figsize=(12, 10))  # Adjust the figsize as needed\n",
    "sns.heatmap(high_corr, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title('High Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mann-Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if features are normally distributed\n",
    "import pandas as pd\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Dictionaries to store the results\n",
    "normal_features = []\n",
    "non_normal_features = []\n",
    "\n",
    "# Iterate through each feature in the DataFrame\n",
    "for feature in all_features:\n",
    "    stat, p_value = shapiro(df[feature])\n",
    "    \n",
    "    # Determine if the feature is normally distributed\n",
    "    if p_value >= 0.05:\n",
    "        normal_features.append(feature)\n",
    "    else:\n",
    "        non_normal_features.append(feature)\n",
    "\n",
    "# Print the results\n",
    "print('Number of normally distributed features:', len(normal_features))        \n",
    "\n",
    "# print(\"Normally Distributed Features:\")\n",
    "# for feature in normal_features:\n",
    "#     print(f\" - {feature}\")\n",
    "\n",
    "# print(\"\\nNon-Normally Distributed Features:\")\n",
    "# for feature in non_normal_features:\n",
    "#     print(f\" - {feature}\")\n",
    "\n",
    "# ALL features are non-normally distributed, therefore we use Man-Whitney Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the shapes to see if they're similar (Optional)\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Assuming df is your DataFrame, and 'group_label' is the column that indicates the group (e.g., male vs. female)\n",
    "male_df = df[df['Gender'] == 'M']\n",
    "female_df = df[df['Gender'] == 'F']\n",
    "\n",
    "# Lists to store results\n",
    "similar_shape_features = []\n",
    "non_similar_shape_features = []\n",
    "\n",
    "# Iterate through each feature in the DataFrame (excluding the group label column)\n",
    "for feature in df.columns:\n",
    "    if feature != 'Gender':  # Skip the group label column\n",
    "        stat, p_value = ks_2samp(male_df[feature], female_df[feature])\n",
    "        \n",
    "        # Determine if the feature has a similar distribution shape\n",
    "        if p_value >= 0.05:\n",
    "            similar_shape_features.append(feature)\n",
    "        else:\n",
    "            non_similar_shape_features.append(feature)\n",
    "\n",
    "# Print the results\n",
    "print(\"Features with Similar Shape:\")\n",
    "for feature in similar_shape_features:\n",
    "    print(f\" - {feature}\")\n",
    "\n",
    "print(\"\\nFeatures with Non-Similar Shape:\")\n",
    "for feature in non_similar_shape_features:\n",
    "    print(f\" - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the df according to gender groups\n",
    "male_df = df[df['Gender'] == 'M']\n",
    "female_df = df[df['Gender'] == 'F']\n",
    "\n",
    "# male_df.to_csv('male_df_for_r.csv')\n",
    "# female_df.to_csv('female_df_for_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, norm\n",
    "\n",
    "# Dictionaries to store results\n",
    "significant_features = {}\n",
    "insignificant_features = {}\n",
    "\n",
    "# Iterate through each feature in the DataFrame\n",
    "for feature in all_features:\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_value = mannwhitneyu(female_df[feature], male_df[feature])\n",
    "    \n",
    "    # Calculate the effect size (Rank-Biserial Correlation)\n",
    "    n1 = len(female_df[feature])\n",
    "    n2 = len(male_df[feature])\n",
    "    rank_biserial = ((2 * u_stat) / (n1 * n2)) - 1\n",
    "    \n",
    "    # Calculate 95% CI for the effect size\n",
    "    z = norm.ppf(1 - 0.05 / 2)  # z-value for 95% confidence\n",
    "    \n",
    "    # Use the absolute value of the effect size for the SE calculation\n",
    "    se = np.sqrt((abs(rank_biserial) * (1 - abs(rank_biserial))) / (n1 * n2))\n",
    "    ci_lower = rank_biserial - z * se\n",
    "    ci_upper = rank_biserial + z * se\n",
    "    \n",
    "    # Store results based on the significance level\n",
    "    if p_value < 0.05:  # Consider the feature significant if p-value < 0.05\n",
    "        significant_features[feature] = {\n",
    "            'u_stat': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'effect_size': rank_biserial,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        }\n",
    "    else:\n",
    "        insignificant_features[feature] = {\n",
    "            'u_stat': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'effect_size': rank_biserial,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        }\n",
    "\n",
    "# # Print the significant features\n",
    "# print(\"Significant Features (p-value < 0.05):\")\n",
    "# for feature, (u_stat, p_value) in significant_features.items():\n",
    "#     print(f\"{feature}: U-stat = {u_stat}, p-value = {p_value}\")\n",
    "\n",
    "# print(\"\\nInsignificant Features (p-value >= 0.05):\")\n",
    "# for feature, (u_stat, p_value) in insignificant_features.items():\n",
    "#     print(f\"{feature}: U-stat = {u_stat}, p-value = {p_value}\")\n",
    "            \n",
    "print('Number of significant features:', len(significant_features))\n",
    "print('Number of insignificant features:', len(insignificant_features))\n",
    "\n",
    "sig_feature_list = list(significant_features.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(value):\n",
    "    if abs(value) < 0.01:  # Use scientific notation for very small numbers\n",
    "        return f\"{value:.2e}\"\n",
    "    elif value >= 1:  # One decimal place for large numbers\n",
    "        return f\"{value:.1f}\"\n",
    "    else:  # Two decimal places for medium-sized numbers\n",
    "        return f\"{value:.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features_df = pd.DataFrame(significant_features).transpose()\n",
    "significant_features_df = significant_features_df.applymap(format_number)\n",
    "significant_features_df.head()\n",
    "\n",
    "significant_features_df.to_csv('significant_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics of central tendency\n",
    "\n",
    "def get_stat(df, feature_list):\n",
    "\n",
    "    # Create a new dictionary to store the mean and IQR for each feature\n",
    "    statistics_dict = {}\n",
    "\n",
    "    for feature in feature_list:\n",
    "        data = df[feature]\n",
    "        \n",
    "        # Calculate mean and IQR\n",
    "        mean_value = np.mean(data)\n",
    "        median_value = np.median(data)\n",
    "        iqr_value = np.percentile(data, 75) - np.percentile(data, 25)\n",
    "        sd_value = np.std(data, ddof=1)\n",
    "        \n",
    "        # Store the results in the dictionary\n",
    "        statistics_dict[feature] = {'mean': mean_value, 'SD': sd_value, 'median': median_value, 'IQR': iqr_value}\n",
    "\n",
    "        stat_df = pd.DataFrame(statistics_dict).transpose()\n",
    "\n",
    "    return stat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_feature_list = list(significant_features.keys())\n",
    "\n",
    "# Get central tendency of female features\n",
    "female_stat_df = get_stat(female_df, sig_feature_list)\n",
    "female_stat_df = female_stat_df.applymap(format_number)\n",
    "display(female_stat_df.head())\n",
    "\n",
    "# Get central tendency of male features\n",
    "male_stat_df = get_stat(male_df, sig_feature_list)\n",
    "male_stat_df = male_stat_df.applymap(format_number)\n",
    "display(male_stat_df.head())\n",
    "\n",
    "# Output as CSV files\n",
    "female_stat_df.to_csv('female_stat.csv')\n",
    "male_stat_df.to_csv('male_stat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Features\n",
    "# Get central tendency of female features\n",
    "female_stat_df = get_stat(female_df, all_features)\n",
    "female_stat_df = female_stat_df.applymap(format_number)\n",
    "display(female_stat_df.head())\n",
    "\n",
    "# Get central tendency of male features\n",
    "male_stat_df = get_stat(male_df, all_features)\n",
    "male_stat_df = male_stat_df.applymap(format_number)\n",
    "display(male_stat_df.head())\n",
    "\n",
    "# Output as CSV files\n",
    "female_stat_df.to_csv('female_stat_all.csv')\n",
    "male_stat_df.to_csv('male_stat_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, norm\n",
    "\n",
    "# Dictionaries to store results\n",
    "features_stat = {}\n",
    "\n",
    "# Iterate through each feature in the DataFrame\n",
    "for feature in all_features:\n",
    "    \n",
    "    # Perform the Mann-Whitney U test\n",
    "    u_stat, p_value = mannwhitneyu(female_df[feature], male_df[feature])\n",
    "    \n",
    "    # Calculate the effect size (Rank-Biserial Correlation)\n",
    "    n1 = len(female_df[feature])\n",
    "    n2 = len(male_df[feature])\n",
    "    rank_biserial = ((2 * u_stat) / (n1 * n2)) - 1\n",
    "    \n",
    "    # Calculate 95% CI for the effect size\n",
    "    z = norm.ppf(1 - 0.05 / 2)  # z-value for 95% confidence\n",
    "    \n",
    "    # Use the absolute value of the effect size for the SE calculation\n",
    "    se = np.sqrt((abs(rank_biserial) * (1 - abs(rank_biserial))) / (n1 * n2))\n",
    "    ci_lower = rank_biserial - z * se\n",
    "    ci_upper = rank_biserial + z * se\n",
    "    \n",
    "    # Store results\n",
    "    features_stat[feature] = {\n",
    "        'u_stat': u_stat,\n",
    "        'p_value': p_value,\n",
    "        'effect_size': rank_biserial,\n",
    "        'ci_lower': ci_lower,\n",
    "        'ci_upper': ci_upper\n",
    "    }\n",
    "\n",
    "features_df = pd.DataFrame(features_stat).transpose()\n",
    "features_df = features_df.applymap(format_number)\n",
    "features_df.head()\n",
    "\n",
    "features_df.to_csv('features_stat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the non-significant features for better performances\n",
    "insig_features_list = list(insignificant_features.keys())\n",
    "\n",
    "print('Number of Insignificant Features:', len(insig_features_list))\n",
    "insig_features_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_exclude = []\n",
    "\n",
    "# Correlation Test\n",
    "features_exclude.append('F2 Character Count')\n",
    "\n",
    "# Mann-Whitney Test\n",
    "features_exclude += insig_features_list\n",
    "# 0 in IQR\n",
    "extra_features = ['F9 EX', 'F10 FW', 'F13 JJR', 'F14 JJS', 'F26 RBR', 'F29 SYM', 'F38 WDT', 'F48 ;']\n",
    "features_exclude += extra_features\n",
    "\n",
    "# Removing Formality Score\n",
    "# f = 'F51 Formality Score'\n",
    "# features_exclude.append(f)\n",
    "\n",
    "# Feature Importance (no better result to remove least important features)\n",
    "# features_exclude = features_exclude + least_important_features\n",
    "\n",
    "features_exclude = list(set(features_exclude))\n",
    "print('Number of total features to exclude:', len(features_exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the non-significant features for better performances\n",
    "selected_features = list(set(all_features) - set(features_exclude))\n",
    "\n",
    "print('Number of Selected Features:', len(selected_features))\n",
    "\n",
    "# Sorting the list numerically based on the number after 'F'\n",
    "selected_features = sorted(selected_features, key=lambda x: int(x.split()[0][1:]))\n",
    "selected_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X = df_bert['Cleaned Content']  # Replace with your actual text column\n",
    "y = df_bert['Gender'].map({'F': 0, 'M': 1})\n",
    "\n",
    "# Under sample the data before converting to BERT embeddings\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X.values.reshape(-1, 1), y)\n",
    "\n",
    "# Flatten the resampled data back to a 1D array\n",
    "X_resampled = X_resampled.flatten()\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to get BERT embeddings for a single text input\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    # Use the embeddings of the [CLS] token\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    return cls_embeddings.numpy()\n",
    "\n",
    "# Apply BERT embedding extraction to the resampled data\n",
    "bert_embeddings = [get_bert_embeddings(text) for text in X_resampled]\n",
    "\n",
    "# Convert the list of embeddings into a DataFrame\n",
    "bert_embeddings_df = pd.DataFrame(bert_embeddings)\n",
    "\n",
    "# Align the index with the resampled y labels for any further use\n",
    "bert_embeddings_df.index = y_resampled.index\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(bert_embeddings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Validation into BERT Embedding\n",
    "df_validation_bert['BERT_Embedding'] = df_validation_bert['Cleaned Content'].apply(get_bert_embeddings)\n",
    "\n",
    "# Convert the embeddings into a DataFrame\n",
    "bert_embeddings_df_val = pd.DataFrame(df_validation_bert['BERT_Embedding'].tolist(), index=df_validation.index)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "display(bert_embeddings_df_val.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Prepare the data\n",
    "X_train, y_train = bert_embeddings_df, y_resampled\n",
    "\n",
    "# Check the distribution of male and female in the training set\n",
    "print(\"Distribution in y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Train a simple logistic regression model\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_val, y_val = bert_embeddings_df_val, df_validation['Gender'].map({'F': 0, 'M': 1})\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under-sampling before feature selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming df is your dataframe containing features and target variable\n",
    "X_train = df[selected_features]\n",
    "y_train = df['Gender_mapped']\n",
    "\n",
    "# Check the distribution of male and female in the training set\n",
    "print(\"Distribution in y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Handle the class imbalance using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the distribution of male and female in the resampled training set\n",
    "print(\"Distribution in y_train_resampled:\")\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "                       min_samples_leaf=4, random_state=42)\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# df_samples = your dataframe containing features and target variable\n",
    "X_val = df_validation[selected_features]\n",
    "# X_val = df_validation[X_features]\n",
    "y_val = df_validation['Gender_mapped']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Search for the best model for under_rf_classifier\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the classifier\n",
    "rf_classifier = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1_macro')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get the best estimator\n",
    "best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "# Train the model with the best estimator\n",
    "best_rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
    "#                        min_samples_leaf=4, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the best model\n",
    "cv_scores = cross_val_score(best_rf_classifier, X_train_resampled, y_train_resampled, cv=5, scoring='accuracy')\n",
    "\n",
    "# Output the cross-validation scores and the mean score\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Perform cross-validation and calculate F1 score\n",
    "f1_scores = cross_val_score(best_rf_classifier, X_train_resampled, y_train_resampled, cv=5, scoring='f1')\n",
    "\n",
    "# Output the cross-validation F1 scores and the mean F1 score\n",
    "print(f\"Cross-validation F1 scores: {f1_scores}\")\n",
    "print(f\"Mean cross-validation F1 score: {f1_scores.mean():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_samples = your dataframe containing features and target variable\n",
    "X_val = df_validation[selected_features]\n",
    "# X_val = df_validation[X_features]\n",
    "y_val = df_validation['Gender_mapped']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Under-sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Assuming df is your dataframe containing features and target variable\n",
    "X_train = df[selected_features]\n",
    "y_train = df['Gender']\n",
    "\n",
    "num_features = len(X_train.columns)\n",
    "\n",
    "print('Number of Features:', num_features)\n",
    "\n",
    "# Check the distribution of male and female in the training set\n",
    "print(\"Distribution in y_train:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Handle the class imbalance using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the distribution of male and female in the resampled training set\n",
    "print(\"Distribution in y_train_resampled:\")\n",
    "print(y_train_resampled.value_counts())\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "under_rf_classifier = RandomForestClassifier(class_weight='balanced', max_depth=30,\n",
    "                       min_samples_split=10, n_estimators=300, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "under_rf_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# df_samples = your dataframe containing features and target variable\n",
    "X_val = df_validation[selected_features]\n",
    "# X_val = df_validation[X_features]\n",
    "y_val = df_validation['Gender']\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = under_rf_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our trained model\n",
    "# under_rf_classifier\n",
    "\n",
    "# Predict probabilities for the validation or test set\n",
    "y_prob = under_rf_classifier.predict_proba(X_val)[:, 1]  # Probabilities for the positive class\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_mapped = y_val.map({'F': 0, 'M': 1})\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr, tpr, thresholds = roc_curve(y_val_mapped, y_prob)\n",
    "\n",
    "# Predict probabilities for the positive class\n",
    "y_prob = under_rf_classifier.predict_proba(X_val)[:, 1]  # Probabilities for the 'M' class\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_val_mapped, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val_mapped, y_prob)\n",
    "\n",
    "# Compute the Youden's J statistic for each threshold\n",
    "j_scores = tpr - fpr\n",
    "\n",
    "# Find the threshold that maximizes the J statistic\n",
    "best_threshold_index = j_scores.argmax()\n",
    "best_threshold = thresholds[best_threshold_index]\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import brentq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_validation['Gender_mapped']\n",
    "\n",
    "# Separate the data based on the actual labels (female=0, male=1)\n",
    "female_probs = y_prob[y_true == 0]\n",
    "male_probs = y_prob[y_true == 1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# KDE plot for females (label = 0)\n",
    "sns.kdeplot(female_probs, fill=True, color='skyblue', label='Female', bw_adjust=1.0)\n",
    "\n",
    "# KDE plot for males (label = 1)\n",
    "sns.kdeplot(male_probs, fill=True, color='orange', label='Male', bw_adjust=1.0)\n",
    "\n",
    "# Labeling and formatting\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('KDE Plot of Predicted Probabilities by Gender')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the intersection point\n",
    "\n",
    "# Generate KDEs for both classes\n",
    "kde_0 = stats.gaussian_kde(female_probs)\n",
    "kde_1 = stats.gaussian_kde(male_probs)\n",
    "\n",
    "# Define the range of values where the intersection might occur\n",
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Get the KDE values for both classes\n",
    "kde_0_vals = kde_0(x)\n",
    "kde_1_vals = kde_1(x)\n",
    "\n",
    "# Find the intersection points\n",
    "def find_intersection(kde1, kde2, x_range):\n",
    "    return brentq(lambda x: kde1(x) - kde2(x), x_range[0], x_range[-1])\n",
    "\n",
    "intersection = find_intersection(kde_0, kde_1, [0, 1])\n",
    "\n",
    "print(f'Intersection Point (Threshold): {intersection:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE Plot with decision threshold\n",
    "y_true = df_validation['Gender_mapped']\n",
    "\n",
    "# Separate the data based on the actual labels (female=0, male=1)\n",
    "female_probs = y_prob[y_true == 0]\n",
    "male_probs = y_prob[y_true == 1]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# KDE plot for females (label = 0)\n",
    "sns.kdeplot(female_probs, fill=True, color='skyblue', label='Female', bw_adjust=1.0)\n",
    "\n",
    "# KDE plot for males (label = 1)\n",
    "sns.kdeplot(male_probs, fill=True, color='orange', label='Male', bw_adjust=1.0)\n",
    "\n",
    "# Plot the decision threshold\n",
    "plt.axvline(x=0.47, color='black', linestyle='--', label='Decision Threshold (0.47)')\n",
    "\n",
    "# Labeling and formatting\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('KDE Plot of Predicted Probabilities by Gender')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final RF Model\n",
    "\n",
    "After feature selection and threshold selection, predict again on the val set using the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "X_val = df_validation[selected_features]\n",
    "y_val = df_validation['Gender_mapped']\n",
    "\n",
    "# Predict the probabilities on the validation set\n",
    "y_pred_proba = under_rf_classifier.predict_proba(X_val)[:, 1]  # Probability of the positive class (e.g., 'Male')\n",
    "\n",
    "# Set the custom threshold of 0.47\n",
    "threshold = 0.47\n",
    "y_pred_final = (y_pred_proba >= threshold).astype(int)  # Convert probabilities to binary predictions based on threshold\n",
    "\n",
    "# Evaluate the model with the custom threshold\n",
    "print(\"Accuracy with threshold 0.47:\", accuracy_score(y_val, y_pred_final))\n",
    "print(\"Classification Report with threshold 0.47:\")\n",
    "print(classification_report(y_val, y_pred_final))\n",
    "print(\"Confusion Matrix with threshold 0.47:\")\n",
    "print(confusion_matrix(y_val, y_pred_final))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Assuming 'under_rf_classifier' is your trained model and 'X_val' is your validation set\n",
    "explainer = shap.TreeExplainer(under_rf_classifier)\n",
    "\n",
    "# Calculate SHAP values for the validation set\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "shap_values_class_1 = shap_values[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP beeswarm plot for class 1 (male) with top 16 features\n",
    "shap.summary_plot(shap_values_class_1, X_val, feature_names=selected_features, plot_type=\"dot\", max_display=16)\n",
    "# plt.title('Top 16 SHAP Values for Male Class (Class 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate mean absolute SHAP values for each feature (assuming binary classification)\n",
    "mean_abs_shap_values_class_0 = np.mean(np.abs(shap_values[:,:,0]), axis=0)\n",
    "mean_abs_shap_values_class_1 = np.mean(np.abs(shap_values[:,:,1]), axis=0)\n",
    "\n",
    "# Average SHAP values across both classes\n",
    "mean_abs_shap_values = (mean_abs_shap_values_class_0 + mean_abs_shap_values_class_1) / 2\n",
    "\n",
    "# Create a DataFrame to display feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_val.columns,\n",
    "    'Mean Absolute SHAP Value': mean_abs_shap_values\n",
    "})\n",
    "\n",
    "# Sort by mean absolute SHAP value\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Mean Absolute SHAP Value', ascending=False)\n",
    "\n",
    "# Display the top significant features\n",
    "print(\"Top Significant Features Based on SHAP Values:\")\n",
    "print(feature_importance_df.head(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_features = feature_importance_df['Feature'][:16].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df.to_csv('shap_feature_importance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative importance\n",
    "feature_importance_df['Cumulative Importance'] = feature_importance_df['Mean Absolute SHAP Value'].cumsum() / feature_importance_df['Mean Absolute SHAP Value'].sum()\n",
    "\n",
    "# Determine number of features to reach 80% cumulative importance\n",
    "threshold = 0.80\n",
    "n_top_features = feature_importance_df[feature_importance_df['Cumulative Importance'] <= threshold].shape[0]\n",
    "\n",
    "print(f\"Number of features required to reach {threshold*100}% cumulative importance: {n_top_features}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Assume y_val_mapped and y_final_pred are defined\n",
    "cm = confusion_matrix(y_val, y_pred_final)\n",
    "labels = ['Female', 'Male']\n",
    "\n",
    "# Normalize the confusion matrix to percentages\n",
    "conf_matrix_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Create the figure and axis\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "custom_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_blues\", ['#deebf7', '#2171b5', '#08306b'], N=256)\n",
    "\n",
    "# Create the heatmap with square cells\n",
    "ax = sns.heatmap(conf_matrix_normalized, annot=False, fmt='.2%', cmap=custom_cmap, xticklabels=labels, yticklabels=labels, square=True)\n",
    "\n",
    "# Annotate the heatmap with both percentages, counts, and classification labels (TP, TN, FP, FN)\n",
    "label_annotations = [['TN', 'FP'], ['FN', 'TP']]  # Define the labels\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        color = 'white' if i == j else 'black'  # Use white text for TN and TP\n",
    "        # Add a separate text for the label (TN, TP, FN, FP) with bold and larger font\n",
    "        plt.text(\n",
    "            j + 0.5, \n",
    "            i + 0.4, \n",
    "            f'{label_annotations[i][j]}',\n",
    "            ha='center', \n",
    "            va='center',  # Align the label at the top of the cell\n",
    "            color=color, \n",
    "            fontsize=14,  # Increase font size for the label\n",
    "            fontweight='bold'  # Make the label bold\n",
    "        )\n",
    "        plt.text(\n",
    "            j + 0.5, \n",
    "            i + 0.5, \n",
    "            f'\\n{conf_matrix_normalized[i, j]:.1%}\\n(n={cm[i, j]:,})',\n",
    "            ha='center', \n",
    "            va='center', \n",
    "            color=color, \n",
    "            fontsize=12  # General font size for text\n",
    "        )\n",
    "\n",
    "\n",
    "# Retrieve the color bar associated with the heatmap\n",
    "colorbar = ax.collections[0].colorbar\n",
    "\n",
    "# Update the color bar to show percentages\n",
    "colorbar.set_ticks([0.3, 0.4, 0.5, 0.6, 0.7])  # Set tick positions in terms of the normalized values\n",
    "colorbar.set_ticklabels(['30%', '40%', '50%', '60%', '70%'])  # Set the labels to percentage format\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "\n",
    "# Retrieve the color patches from the heatmap\n",
    "patches = ax.collections[0].get_paths()\n",
    "\n",
    "# Extract the color for each cell\n",
    "colors = [ax.collections[0].get_facecolor()[i] for i in range(len(patches))]\n",
    "\n",
    "# Map the colors to the corresponding labels\n",
    "color_map = {\n",
    "    'TN': colors[0],  # Top-left\n",
    "    'FP': colors[1],  # Top-right\n",
    "    'FN': colors[2],  # Bottom-left\n",
    "    'TP': colors[3]   # Bottom-right\n",
    "}\n",
    "\n",
    "# Custom legend-like explanation with square markers using the extracted colors\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color=color_map['TN'], marker='s', markersize=10, linestyle='None', label='TN: True Negative\\n(Females correctly classified as Females)'),\n",
    "    plt.Line2D([0], [0], color=color_map['FP'], marker='s', markersize=10, linestyle='None', label='FP: False Positive\\n(Females incorrectly classified as Males)'),\n",
    "    plt.Line2D([0], [0], color=color_map['FN'], marker='s', markersize=10, linestyle='None', label='FN: False Negative\\n(Males incorrectly classified as Females)'),\n",
    "    plt.Line2D([0], [0], color=color_map['TP'], marker='s', markersize=10, linestyle='None', label='TP: True Positive\\n(Males correctly classified as Males)')\n",
    "]\n",
    "\n",
    "\n",
    "# Position the custom legend to the right of the plot\n",
    "plt.figlegend(handles=legend_elements, loc='center left', bbox_to_anchor=(0.9, 0.5), borderaxespad=0., frameon=False, labelspacing=2)\n",
    "\n",
    "# Adjust layout to prevent clipping\n",
    "plt.tight_layout(rect=[0, 0, 0,0])  # Adjust the layout to leave space for the legend\n",
    "\n",
    "plt.savefig('cm_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted classes (assuming the model was trained for multi-class classification)\n",
    "predicted_classes = y_final_pred\n",
    "\n",
    "df_validation_class = df_validation.copy()\n",
    "\n",
    "# Add these classified classes back into your DataFrame if you want\n",
    "df_validation_class['Predicted Gender'] = predicted_classes\n",
    "\n",
    "\n",
    "# Define a function to classify each instance into one of the 4 classes\n",
    "def classify_confusion_matrix(actual, predicted):\n",
    "    if actual == 1 and predicted == 1:\n",
    "        return 'TP'  # TP\n",
    "    elif actual == 0 and predicted == 0:\n",
    "        return 'TN'  # TN\n",
    "    elif actual == 0 and predicted == 1:\n",
    "        return 'FP' # FP\n",
    "    elif actual == 1 and predicted == 0:\n",
    "        return 'FN' # FN\n",
    "    else:\n",
    "        return 'Unknown'  # Just in case there's some data issue\n",
    "\n",
    "# Apply the function across the DataFrame\n",
    "df_validation_class['Confusion Matrix Class'] = df_validation_class.apply(\n",
    "    lambda row: classify_confusion_matrix(row['Gender_mapped'], row['Predicted Gender']), axis=1\n",
    ")\n",
    "\n",
    "# Output the classified groups\n",
    "# True Positive (TP): Actual Male (1) and Predicted Male (1)\n",
    "df_validation_tp = df_validation_class[df_validation_class['Confusion Matrix Class'] == 'TP']\n",
    "\n",
    "# True Negative (TN): Actual Female (0) and Predicted Female (0)\n",
    "df_validation_tn = df_validation_class[df_validation_class['Confusion Matrix Class'] == 'TN']\n",
    "\n",
    "# False Positive (FP): Actual Female (0) but Predicted Male (1)\n",
    "df_validation_fp = df_validation_class[df_validation_class['Confusion Matrix Class'] == 'FP']\n",
    "\n",
    "# False Negative (FN): Actual Male (1) but Predicted Female (0)\n",
    "df_validation_fn = df_validation_class[df_validation_class['Confusion Matrix Class'] == 'FN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df_validation_class_standardised = df_validation_class.copy()\n",
    "\n",
    "# # Normalize the features (scales all features to [0, 1])\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# df_validation_class_normalised[most_important_features] = scaler.fit_transform(df_validation_class[most_important_features])\n",
    "\n",
    "# OR Standardize the features (scales to mean=0 and std=1)\n",
    "scaler = StandardScaler()\n",
    "df_validation_class_standardised[shap_features] = scaler.fit_transform(df_validation_class[shap_features])\n",
    "\n",
    "# Assuming df_validation_class is your DataFrame and it has a 'Confusion Matrix Class' column\n",
    "# Also assuming your features are stored in a separate list `feature_columns`\n",
    "mean_values = {}\n",
    "\n",
    "for label in ['TP', 'TN', 'FP', 'FN']:\n",
    "    mean_values[label] = df_validation_class_standardised[df_validation_class_standardised['Confusion Matrix Class'] == label][shap_features].mean()\n",
    "\n",
    "mean_df = pd.DataFrame(mean_values)\n",
    "\n",
    "# Calculate median values for all positives (true and false positives)\n",
    "mean_positives = df_validation_class_standardised[df_validation_class_standardised['Gender_mapped'] == 1][shap_features].mean()\n",
    "\n",
    "# Calculate median values for all negatives (true and false negatives)\n",
    "mean_negatives = df_validation_class_standardised[df_validation_class_standardised['Gender_mapped'] == 0][shap_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transpose the DataFrame so features are on the x-axis\n",
    "mean_df_transposed = mean_df.T\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "# Define the categories you want to plot\n",
    "categories_to_plot = ['TP', 'TN', 'FP', 'FN']  # Female vs Male classified as Female\n",
    "\n",
    "# Define line styles for each category\n",
    "line_styles = {\n",
    "    'TN': '-',  # Solid line for True Negative\n",
    "    'TP': '-',  # Solid line for True Positive\n",
    "    'FN': '--', # Dashed line for False Negative\n",
    "    'FP': '--'  # Dashed line for False Positive\n",
    "}\n",
    "\n",
    "# Define colors for each category\n",
    "colors = {\n",
    "    'TN': 'red',    # Blue for True Negative\n",
    "    'TP': 'green',   # Green for True Positive\n",
    "    'FN': 'orange',     # Red for False Negative\n",
    "    'FP': 'royalblue'   # Orange for False Positive\n",
    "}\n",
    "\n",
    "# Define alpha values for each category\n",
    "alphas = {\n",
    "    'TN': 1,\n",
    "    'TP': 1,\n",
    "    'FN': 1,\n",
    "    'FP': 1\n",
    "}\n",
    "\n",
    "# Plotting the medians for each category with the specified line style and color\n",
    "for category in categories_to_plot:\n",
    "    plt.plot(mean_df_transposed.columns, mean_df_transposed.loc[category], \n",
    "             marker='o', linestyle=line_styles[category], color=colors[category], label=category, alpha=alphas[category])\n",
    "\n",
    "# Increase font size for labels, title, and legend\n",
    "plt.xlabel('Features', fontsize=20)\n",
    "plt.ylabel('Mean Standardised Feature Value', fontsize=20)\n",
    "# plt.title('Mean Standardised Feature Values of All Classes', fontsize=22)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)  # Rotate feature names if necessary\n",
    "plt.yticks(fontsize=16)  # Set fontsize for y-axis ticks\n",
    "plt.legend(fontsize=16)  # Set fontsize for the legend\n",
    "plt.tight_layout()  # Adjusts layout to prevent clipping of tick-labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transpose the DataFrame so features are on the x-axis\n",
    "mean_df_transposed = mean_df.T\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "# Define the categories you want to plot\n",
    "categories_to_plot = ['TP', 'TN', 'FP', 'FN']  # Female vs Male classified as Female\n",
    "\n",
    "# Define line styles for each category\n",
    "line_styles = {\n",
    "    'TN': '-',  # Solid line for True Negative\n",
    "    'TP': '-',  # Solid line for True Positive\n",
    "    'FN': '--', # Dashed line for False Negative\n",
    "    'FP': '--'  # Dashed line for False Positive\n",
    "}\n",
    "\n",
    "# Define colors for each category\n",
    "colors = {\n",
    "    'TN': 'red',    # Blue for True Negative\n",
    "    'TP': 'green',   # Green for True Positive\n",
    "    'FN': 'orange',     # Red for False Negative\n",
    "    'FP': 'royalblue'   # Orange for False Positive\n",
    "}\n",
    "\n",
    "# Define alpha values for each category\n",
    "alphas = {\n",
    "    'TN': 1,\n",
    "    'TP': 0.15,\n",
    "    'FN': 1,\n",
    "    'FP': 0.15\n",
    "}\n",
    "\n",
    "# Plotting the medians for each category with the specified line style and color\n",
    "for category in categories_to_plot:\n",
    "    plt.plot(mean_df_transposed.columns, mean_df_transposed.loc[category], \n",
    "             marker='o', linestyle=line_styles[category], color=colors[category], label=category, alpha=alphas[category])\n",
    "\n",
    "\n",
    "# Create a legend with only TN and FN\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "selected_handles_labels = [(h, l) for h, l in zip(handles, labels) if l in ['TN', 'FN']]\n",
    "if selected_handles_labels:\n",
    "    handles, labels = zip(*selected_handles_labels)\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "plt.xlabel('Features', fontsize=20)\n",
    "plt.ylabel('Mean Standardised Feature Value', fontsize=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)  # Rotate feature names if necessary\n",
    "plt.yticks(fontsize=16)  # Set fontsize for y-axis ticks\n",
    "plt.legend(fontsize=16)  # Set fontsize for the legend\n",
    "plt.tight_layout()  # Adjusts layout to prevent clipping of tick-labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Transpose the DataFrame so features are on the x-axis\n",
    "mean_df_transposed = mean_df.T\n",
    "\n",
    "plt.figure(figsize=(22, 10))\n",
    "\n",
    "# Define the categories you want to plot\n",
    "categories_to_plot = ['TP', 'TN', 'FP', 'FN']  # Female vs Male classified as Female\n",
    "\n",
    "# Define line styles for each category\n",
    "line_styles = {\n",
    "    'TN': '-',  # Solid line for True Negative\n",
    "    'TP': '-',  # Solid line for True Positive\n",
    "    'FN': '--', # Dashed line for False Negative\n",
    "    'FP': '--'  # Dashed line for False Positive\n",
    "}\n",
    "\n",
    "# Define colors for each category\n",
    "colors = {\n",
    "    'TN': 'red',    # Blue for True Negative\n",
    "    'TP': 'green',   # Green for True Positive\n",
    "    'FN': 'orange',     # Red for False Negative\n",
    "    'FP': 'royalblue'   # Orange for False Positive\n",
    "}\n",
    "\n",
    "# Define alpha values for each category\n",
    "alphas = {\n",
    "    'TN': 0.15,\n",
    "    'TP': 1,\n",
    "    'FN': 0.15,\n",
    "    'FP': 1\n",
    "}\n",
    "\n",
    "# Plotting the medians for each category with the specified line style and color\n",
    "for category in categories_to_plot:\n",
    "    plt.plot(mean_df_transposed.columns, mean_df_transposed.loc[category], \n",
    "             marker='o', linestyle=line_styles[category], color=colors[category], label=category, alpha=alphas[category])\n",
    "\n",
    "# Create a legend with only TN and FN\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "selected_handles_labels = [(h, l) for h, l in zip(handles, labels) if l in ['TP', 'FP']]\n",
    "if selected_handles_labels:\n",
    "    handles, labels = zip(*selected_handles_labels)\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "plt.xlabel('Features', fontsize=20)\n",
    "plt.ylabel('Mean Standardised Feature Value', fontsize=20)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16)  # Rotate feature names if necessary\n",
    "plt.yticks(fontsize=16)  # Set fontsize for y-axis ticks\n",
    "plt.legend(fontsize=16)  # Set fontsize for the legend\n",
    "plt.tight_layout()  # Adjusts layout to prevent clipping of tick-labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_feature = get_stat(df_validation_tp, shap_features)\n",
    "tn_feature = get_stat(df_validation_tn, shap_features)\n",
    "fp_feature = get_stat(df_validation_fp, shap_features)\n",
    "fn_feature = get_stat(df_validation_fn, shap_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_feature.applymap(format_number)\n",
    "tn_feature.applymap(format_number)\n",
    "fp_feature.applymap(format_number)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled_normalized = scaler.fit_transform(X_train_resampled)\n",
    "\n",
    "# Convert target variable 'Gender' to numeric\n",
    "y_train_resampled_encoded = y_train_resampled.map({'F': 0, 'M': 1})\n",
    "\n",
    "# Initialize the Lasso model with a chosen alpha (regularization strength)\n",
    "lasso = LogisticRegression(C=10, penalty='l1', random_state=42, solver='liblinear')\n",
    "\n",
    "# Train the model\n",
    "lasso.fit(X_train_resampled_normalized, y_train_resampled_encoded)\n",
    "\n",
    "# Predict and evaluate\n",
    "# df_samples = your dataframe containing features and target variable\n",
    "X_val = df_validation.drop(features_exclude, axis=1)\n",
    "X_val_normalized = scaler.fit_transform(X_val)\n",
    "y_val = df_validation['Gender']\n",
    "y_val_encoded = y_val.map({'F': 0, 'M': 1})\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = lasso.predict(X_val_normalized)\n",
    "\n",
    "# Since Lasso is a regression model, we need to convert predictions to binary outcomes\n",
    "# A common approach is to use a threshold of 0.5\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val_encoded, y_val_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the Lasso model\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "lasso.fit(X_train_resampled_normalized, y_train_resampled_encoded)\n",
    "\n",
    "# Predict probabilities (output between 0 and 1)\n",
    "y_pred_proba = lasso.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "# Add predicted probabilities to the test set for analysis\n",
    "test_results = X_val.copy()\n",
    "test_results['Actual'] = y_val_encoded\n",
    "test_results['Predicted_Prob'] = y_pred_proba\n",
    "\n",
    "print(test_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Lasso model\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "lasso.fit(X_train_resampled_normalized, y_train_resampled_encoded)\n",
    "\n",
    "# Predict on the test set\n",
    "y_val_pred = lasso.predict(X_val_normalized)\n",
    "\n",
    "# Since Lasso is a regression model, we need to convert predictions to binary outcomes\n",
    "# A common approach is to use a threshold of 0.5\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val_encoded, y_val_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n",
    "    'penalty': ['l1'],  # Lasso (L1) penalty\n",
    "    'solver': ['liblinear']  # Solver that supports L1 regularization\n",
    "}\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic = LogisticRegression(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(logistic, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_resampled_normalized, y_train_resampled_encoded)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters found: \", best_params)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_logistic = grid_search.best_estimator_\n",
    "best_logistic.fit(X_train_resampled_normalized, y_train_resampled_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Perform cross-validation on the best model\n",
    "cv_scores = cross_val_score(best_logistic, X_train_resampled_normalized, y_train_resampled_encoded, cv=5, scoring='accuracy')\n",
    "\n",
    "# Output the cross-validation scores and the mean score\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.4f}\")\n",
    "\n",
    "# Perform cross-validation and calculate F1 score\n",
    "f1_scores = cross_val_score(best_logistic, X_train_resampled_normalized, y_train_resampled_encoded, cv=5, scoring='f1')\n",
    "\n",
    "# Output the cross-validation F1 scores and the mean F1 score\n",
    "print(f\"Cross-validation F1 scores: {f1_scores}\")\n",
    "print(f\"Mean cross-validation F1 score: {f1_scores.mean():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_val_pred = best_logistic.predict(X_val_normalized)\n",
    "\n",
    "# Since Lasso is a regression model, we need to convert predictions to binary outcomes\n",
    "# A common approach is to use a threshold of 0.5\n",
    "y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_encoded, y_val_pred_binary))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val_encoded, y_val_pred_binary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients\n",
    "coefficients = best_logistic.coef_[0]\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients})\n",
    "feature_importances = feature_importances.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Display the coefficients\n",
    "print(\"Lasso Regression Coefficients:\")\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and y_train are your training data features and labels\n",
    "X['Gender'] = y  # Add the Gender column back to the features for analysis\n",
    "\n",
    "# Calculate summary statistics for Formality Score\n",
    "formality_stats = X.groupby('Gender')['F10 Formality Score'].describe()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics for Formality Score:\")\n",
    "print(formality_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract the formality score and gender columns\n",
    "formality_scores = X[['F10 Formality Score', 'Gender']]\n",
    "\n",
    "# Standardize the formality scores\n",
    "scaler = StandardScaler()\n",
    "formality_scores['F10 Formality Score Standardized'] = scaler.fit_transform(formality_scores[['F10 Formality Score']])\n",
    "\n",
    "# Calculate summary statistics for the standardized formality scores\n",
    "standardized_stats = formality_scores.groupby('Gender')['F10 Formality Score Standardized'].describe()\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary Statistics for Standardized Formality Score:\")\n",
    "print(standardized_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Example for classification\n",
    "gbm = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "gbm.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = gbm.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5]\n",
    "}\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=gbm, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "# Best parameters found: {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 100, 'subsample': 0.7}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Perform cross-validation and get multiple scores\n",
    "cv_results = cross_validate(gbm_best, X_train_resampled, y_train_resampled, cv=5, \n",
    "                            scoring=['accuracy', 'f1'], return_train_score=False)\n",
    "\n",
    "# Output the cross-validation scores\n",
    "print(f\"Cross-validation accuracy scores: {cv_results['test_accuracy']}\")\n",
    "print(f\"Mean cross-validation accuracy score: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "\n",
    "print(f\"Cross-validation F1 scores: {cv_results['test_f1']}\")\n",
    "print(f\"Mean cross-validation F1 score: {cv_results['test_f1'].mean():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_best = GradientBoostingClassifier(\n",
    "    learning_rate=0.1, \n",
    "    max_depth=5, \n",
    "    min_samples_leaf=5, \n",
    "    min_samples_split=2, \n",
    "    n_estimators=100, \n",
    "    subsample=0.7,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbm_best.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred = gbm_best.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Gradient Boosting Classifier')\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
